{"paragraphs":[{"title":"Prepare, run once per session","text":"z.angularBind(\"input_f\", \"\")\nz.angularBind(\"stop_words\", \"2\") \nz.angularBind(\"num_topics\", \"4\") \nz.angularBind(\"term_per_topic\", \"4\") \nz.angularBind(\"sentiment_output_path\", \"\") ","user":"rhuang","dateUpdated":"2018-05-31T11:11:15-0500","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1519940079704_-870758091","id":"20170304-153234_626460964","dateCreated":"2018-03-01T15:34:39-0600","dateStarted":"2018-05-31T11:11:15-0500","dateFinished":"2018-05-31T11:11:43-0500","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12987"},{"text":"%angular\n\n<form class=\"form-inline\">\n \n  <div class=\"form-group\">\n    <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('input_f', {inputfile : inputfile,from : from_date,end : end_date, stopwords_path : stopwords_path}, '20170302-163418_715093436'); z.runParagraph('20170302-163418_715093436')\"> Load </button> tweets from\n    <input type=\"text\" class=\"form-control\" ng-init=\"inputfile='file:/data/00791/xwj/tweets/tweets/parquet/2017/part-00096-3b88e4ea-3cb6-4efd-9e2e-64914ae08a19.snappy.parquet'\" placeholder=\"input data location\" ng-model=\"inputfile\"></input>\n    between <input type=\"text\" class=\"form-control\" ng-init=\"from_date='2016-01-19 15:00:00'\" placeholder=\"2016-01-19 15:00:00\" ng-model=\"from_date\"></input>\n    and <input type=\"text\" class=\"form-control\" ng-init=\"end_date='2018-01-19 20:00:00'\" placeholder=\"2018-01-19 20:00:00\" ng-model=\"end_date\"></input>\n    stopwords from <input type=\"text\" class=\"form-control\" ng-init=\"stopwords_path='file:/data/00791/xwj/tweets/tweets/stopwords.txt'\" placeholder=\"input stop words location\" ng-model=\"stopwords_path\"></input>\n  </div>\n  <BR>\n  <div class=\"form-group\">\n     <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('stop_words', stop_words, '20170302-205317_1581000322');z.runParagraph('20170302-205317_1581000322')\"> Remove </button>\n     Top <input type=\"number\" class=\"form-control\" ng-init=\"stop_words='2'\" placeholder=2 ng-model=\"stop_words\"></input> frequent words\n   \n  </div>\n  <BR>\n  <div class=\"form-group\">\n    <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('num_topics', num_topics, '20170302-205337_768402024');\n                    z.angularBind('term_per_topic', term_per_topic, '20170302-205337_768402024');\n                    z.runParagraph('20170302-205337_768402024')\"> Run </button> topic modeling for \n    for top <input type=\"number\" class=\"form-control\" ng-init=\"num_topics='3'\" placeholder=3 ng-model=\"num_topics\"></input> topcis\n    with <input type=\"number\" class=\"form-control\" ng-init=\"term_per_topic='3'\" placeholder=3 ng-model=\"term_per_topic\"></input> terms per topic.\n  </div>\n  <BR>\n\n \n  \n</form>\n","user":"rhuang","dateUpdated":"2018-05-31T14:37:01-0500","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/undefined","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"<form class=\"form-inline\">\n \n  <div class=\"form-group\">\n    <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('input_f', {inputfile : inputfile,from : from_date,end : end_date, stopwords_path : stopwords_path}, '20170302-163418_715093436'); z.runParagraph('20170302-163418_715093436')\"> Load </button> tweets from\n    <input type=\"text\" class=\"form-control\" ng-init=\"inputfile='file:/data/00791/xwj/tweets/tweets/parquet/2017/part-00096-3b88e4ea-3cb6-4efd-9e2e-64914ae08a19.snappy.parquet'\" placeholder=\"input data location\" ng-model=\"inputfile\"></input>\n    between <input type=\"text\" class=\"form-control\" ng-init=\"from_date='2016-01-19 15:00:00'\" placeholder=\"2016-01-19 15:00:00\" ng-model=\"from_date\"></input>\n    and <input type=\"text\" class=\"form-control\" ng-init=\"end_date='2018-01-19 20:00:00'\" placeholder=\"2018-01-19 20:00:00\" ng-model=\"end_date\"></input>\n    stopwords from <input type=\"text\" class=\"form-control\" ng-init=\"stopwords_path='file:/data/00791/xwj/tweets/tweets/stopwords.txt'\" placeholder=\"input stop words location\" ng-model=\"stopwords_path\"></input>\n  </div>\n  <BR>\n  <div class=\"form-group\">\n     <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('stop_words', stop_words, '20170302-205317_1581000322');z.runParagraph('20170302-205317_1581000322')\"> Remove </button>\n     Top <input type=\"number\" class=\"form-control\" ng-init=\"stop_words='2'\" placeholder=2 ng-model=\"stop_words\"></input> frequent words\n   \n  </div>\n  <BR>\n  <div class=\"form-group\">\n    <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('num_topics', num_topics, '20170302-205337_768402024');\n                    z.angularBind('term_per_topic', term_per_topic, '20170302-205337_768402024');\n                    z.runParagraph('20170302-205337_768402024')\"> Run </button> topic modeling for \n    for top <input type=\"number\" class=\"form-control\" ng-init=\"num_topics='3'\" placeholder=3 ng-model=\"num_topics\"></input> topcis\n    with <input type=\"number\" class=\"form-control\" ng-init=\"term_per_topic='3'\" placeholder=3 ng-model=\"term_per_topic\"></input> terms per topic.\n  </div>\n  <BR>\n\n \n  \n</form>"}]},"apps":[],"jobName":"paragraph_1519940079710_-871527589","id":"20170302-163514_972951478","dateCreated":"2018-03-01T15:34:39-0600","dateStarted":"2018-05-31T14:37:01-0500","dateFinished":"2018-05-31T14:37:01-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12988"},{"title":"Top terms from data sets","text":"\nimport scala.collection.mutable\nimport org.apache.spark.mllib.clustering.LDA\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql._\n\n//z.angularBind(\"input_f\", \"\") \nval input_f = z.angular(\"input_f\").toString\ninput_f.substring(0, input_f.length).split(\",\")\ninput_f.substring(1, input_f.length-1).split(\",\").map(_.split(\"=\"))\nval para_map = input_f.substring(1, input_f.length-1).split(\",\").map(_.split(\"=\")).map{case Array(k, v) => (k.trim, v.trim)}.toMap\nval inputPath = para_map.getOrElse(\"inputfile\" , null)\nval from_time : String = para_map.getOrElse(\"from\", null)\nval end_time : String  =  para_map.getOrElse(\"end\", null)\nval stopWords_path=para_map.getOrElse(\"stopwords_path\", null)\n\n//read description of each tweets\n//val tweets = spark.read.format(\"csv\").option(\"header\", true).load(inputPath)\nval tweets = spark.read.format(\"parquet\").load(inputPath)\n        .filter('pubdate.between(from_time, end_time))\n//  .select(\"id\", \"guid\", \"link\", \"pubdate\", \"author\", \"title\", \"description\", \"source\", \"postertimezone\",\"user_id\");\n\n\n// Split each document into a sequence of terms (words)\nval stopWords = sc.textFile(stopWords_path)\nval stopWordSet = stopWords.collect.toSet\nval stopWordSetBC = sc.broadcast(stopWordSet)\nval corpus: RDD[String] = tweets.select(\"description\").filter(\"description is not null\").rdd.map(r => r(0).asInstanceOf[String])\nval tokenized: RDD[Seq[String]] =\n  corpus.map(_.toLowerCase.split(\"\\\\s\"))\n        .map(_.filter(_.length > 3)\n             .filter(_.forall(java.lang.Character.isLetter))\n             .filter( w => !stopWordSetBC.value.contains(w)))\n             \n// Show top words found in the corpus\nval termCounts: Array[(String, Long)] = tokenized.flatMap(_.map(_ -> 1L)).reduceByKey(_ + _).collect().sortBy(-_._2)\nvar b= new StringBuilder\ntermCounts.take(30).foreach { case (term, count) => b.append(s\"$term\\t$count\\n\")}\nprintln(termCounts.length + \" unique tokens from \"+ corpus.count + \" tweets.\")\nprint(s\"%table word\\t freq\\n$b\")\n","user":"rhuang","dateUpdated":"2018-05-31T11:13:00-0500","config":{"lineNumbers":true,"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":808,"optionOpen":false}},"1":{"graph":{"mode":"multiBarChart","height":279,"optionOpen":false},"helium":{}}},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport scala.collection.mutable\n\nimport org.apache.spark.mllib.clustering.LDA\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.sql.SQLContext\n\nimport org.apache.spark.sql._\n\ninput_f: String = {inputfile=file:/data/00791/xwj/tweets/tweets/parquet/2017/part-00096-3b88e4ea-3cb6-4efd-9e2e-64914ae08a19.snappy.parquet, from=2016-01-19 15:00:00, end=2018-01-19 20:00:00, stopwords_path=file:/data/00791/xwj/tweets/tweets/stopwords.txt}\n\nres6: Array[String] = Array({inputfile=file:/data/00791/xwj/tweets/tweets/parquet/2017/part-00096-3b88e4ea-3cb6-4efd-9e2e-64914ae08a19.snappy.parquet, \" from=2016-01-19 15:00:00\", \" end=2018-01-19 20:00:00\", \" stopwords_path=file:/data/00791/xwj/tweets/tweets/stopwords.txt}\")\n\nres7: Array[Array[String]] = Array(Array(inputfile, file:/data/00791/xwj/tweets/tweets/parquet/2017/part-00096-3b88e4ea-3cb6-4efd-9e2e-64914ae08a19.snappy.parquet), Array(\" from\", 2016-01-19 15:00:00), Array(\" end\", 2018-01-19 20:00:00), Array(\" stopwords_path\", file:/data/00791/xwj/tweets/tweets/stopwords.txt))\n\npara_map: scala.collection.immutable.Map[String,String] = Map(inputfile -> file:/data/00791/xwj/tweets/tweets/parquet/2017/part-00096-3b88e4ea-3cb6-4efd-9e2e-64914ae08a19.snappy.parquet, from -> 2016-01-19 15:00:00, end -> 2018-01-19 20:00:00, stopwords_path -> file:/data/00791/xwj/tweets/tweets/stopwords.txt)\n\ninputPath: String = file:/data/00791/xwj/tweets/tweets/parquet/2017/part-00096-3b88e4ea-3cb6-4efd-9e2e-64914ae08a19.snappy.parquet\n\nfrom_time: String = 2016-01-19 15:00:00\n\nend_time: String = 2018-01-19 20:00:00\n\nstopWords_path: String = file:/data/00791/xwj/tweets/tweets/stopwords.txt\n\ntweets: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [id: string, guid: string ... 18 more fields]\n\nstopWords: org.apache.spark.rdd.RDD[String] = file:/data/00791/xwj/tweets/tweets/stopwords.txt MapPartitionsRDD[3] at textFile at <console>:43\nstopWordSet: scala.collection.immutable.Set[String] = Set(serious, latterly, down, side, moreover, please, ourselves, behind, for, find, further, mill, due, any, wherein, across, twenty, name, this, in, move, itse\", have, your, off, once, are, is, his, why, too, among, everyone, show, empty, already, nobody, less, am, hence, system, than, four, fire, anyhow, three, whereby, con, twelve, throughout, but, whether, below, co, mine, becomes, eleven, what, would, although, elsewhere, another, front, if, hereby, own, neither, bottom, up, etc, so, our, per, therein, must, beforehand, keep, do, all, him, had, somehow, re, onto, nor, every, herein, full, before, afterwards, somewhere, whither, else, namely, us, it, whereupon, two, thence, a, herse\", sometimes, became, though, within, as, because...\nstopWordSetBC: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] = Broadcast(3)\n\ncorpus: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at map at <console>:47\n\ntokenized: org.apache.spark.rdd.RDD[Seq[String]] = MapPartitionsRDD[10] at map at <console>:61\ntermCounts: Array[(String, Long)] = Array((trump,614), (donald,80), (president,76), (rally,58), (just,51), (people,45), (news,38), (media,35), (press,29), (fake,29), (like,28), (white,27), (want,24), (melania,24), (house,23), (mccain,23), (america,23), (going,22), (sweden,22), (calls,22), (know,21), (florida,21), (think,20), (cost,20), (says,19), (russia,19), (incident,19), (stop,18), (family,18), (using,17), (rallies,16), (americans,16), (security,16), (country,16), (night,15), (black,15), (obama,15), (trust,15), (lies,14), (taxpayers,14), (believe,14), (meme,14), (golf,14), (lemon,14), (companies,14), (american,13), (john,13), (enemy,13), (days,13), (tiny,13), (said,13), (today,13), (travel,13), (attacks,13), (tell,13), (mentions,12), (surrogate,12), (cuts,12), (seamlessly,12), (discu...\nb: StringBuilder =\n2014 unique tokens from 1000 tweets.\n"},{"type":"TABLE","data":"word\t freq\ntrump\t614\ndonald\t80\npresident\t76\nrally\t58\njust\t51\npeople\t45\nnews\t38\nmedia\t35\npress\t29\nfake\t29\nlike\t28\nwhite\t27\nwant\t24\nmelania\t24\nhouse\t23\nmccain\t23\namerica\t23\ngoing\t22\nsweden\t22\ncalls\t22\nknow\t21\nflorida\t21\nthink\t20\ncost\t20\nsays\t19\nrussia\t19\nincident\t19\nstop\t18\nfamily\t18\nusing\t17\n"}]},"apps":[],"jobName":"paragraph_1519940079711_-871912338","id":"20170302-163418_715093436","dateCreated":"2018-03-01T15:34:39-0600","dateStarted":"2018-05-31T11:13:00-0500","dateFinished":"2018-05-31T11:13:12-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12989"},{"title":"Top terms after preprocessing","text":"\n//Preprocessing the documents \n//   vocabArray: Chosen vocab (removing common terms)\n\n//z.angularBind(\"stop_words\", \"20\") \nval numStopwords = z.angular(\"stop_words\").toString.toDouble.toInt\n\n\n\nval vocabArray: Array[String] =\n  termCounts.takeRight(termCounts.size - numStopwords).map(_._1)\n\nprintln(vocabArray.length + \" tokens are used\")\n\n//   vocab: Map term -> term index\nval vocab: Map[String, Int] = vocabArray.zipWithIndex.toMap\n\n// Convert documents into term count vectors\nval documents: RDD[(Long, Vector)] =\n  tokenized.zipWithIndex.map { case (tokens, id) =>\n    val counts = new mutable.HashMap[Int, Double]()\n    tokens.foreach { term =>\n      if (vocab.contains(term)) {\n        val idx = vocab(term)\n        counts(idx) = counts.getOrElse(idx, 0.0) + 1.0\n      }\n    }\n    (id, Vectors.sparse(vocab.size, counts.toSeq))\n  }\n  \nvar b2= new StringBuilder\ntermCounts.take(numStopwords+30).takeRight(30).foreach { case (term, count) => b2.append(s\"$term\\t$count\\n\")}\nprint(s\"%table word\\t freq\\n$b2\") ","user":"rhuang","dateUpdated":"2018-05-31T11:13:27-0500","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":828,"optionOpen":false}},"1":{"graph":{"mode":"multiBarChart","height":348,"optionOpen":false},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nnumStopwords: Int = 3\nvocabArray: Array[String] = Array(rally, just, people, news, media, press, fake, like, white, want, melania, house, mccain, america, going, sweden, calls, know, florida, think, cost, says, russia, incident, stop, family, using, rallies, americans, security, country, night, black, obama, trust, lies, taxpayers, believe, meme, golf, lemon, companies, american, john, enemy, days, tiny, said, today, travel, attacks, tell, mentions, surrogate, cuts, seamlessly, discussing, term, change, thank, needs, happen, support, russian, love, dictators, talking, denial, fuel, calling, campaign, fossil, total, chicago, thing, reporting, spread, favorite, making, secret, holding, protest, national, month, currently, climate, words, voted, looks, saying, open, free, course, nervous, means, march, obamas, ...2011 tokens are used\nvocab: Map[String,Int] = Map(incident -> 23, speaker -> 1475, terrible -> 1947, pres -> 740, rate -> 728, pepper -> 1853, germans -> 1410, michael -> 1573, looks -> 88, california -> 1765, coteries -> 1201, silk -> 710, tweeted -> 115, used -> 189, indication -> 1980, aparentemente -> 1845, allowed -> 1847, secretary -> 1748, stuns -> 1259, aliens -> 1138, beautiful -> 229, sunday -> 259, writing -> 1567, funny -> 1077, launch -> 1296, devastating -> 987, warns -> 594, killed -> 1013, cord -> 1267, interfered -> 1111, bowling -> 640, racist -> 683, prayed -> 1439, perfection -> 1073, entirely -> 879, senator -> 251, kills -> 776, health -> 1257, mommy -> 1012, intenta -> 1844, economy -> 1428, reporters -> 373, messages -> 1211, désigné -> 954, hiring -> 1184, tweeting -> 1596, ties -> ...\ndocuments: org.apache.spark.rdd.RDD[(Long, org.apache.spark.mllib.linalg.Vector)] = MapPartitionsRDD[16] at map at <console>:70\n\nb2: StringBuilder =\n"},{"type":"TABLE","data":"word\t freq\nrally\t58\njust\t51\npeople\t45\nnews\t38\nmedia\t35\npress\t29\nfake\t29\nlike\t28\nwhite\t27\nwant\t24\nmelania\t24\nhouse\t23\nmccain\t23\namerica\t23\ngoing\t22\nsweden\t22\ncalls\t22\nknow\t21\nflorida\t21\nthink\t20\ncost\t20\nsays\t19\nrussia\t19\nincident\t19\nstop\t18\nfamily\t18\nusing\t17\nrallies\t16\namericans\t16\nsecurity\t16\n"}]},"apps":[],"jobName":"paragraph_1519940079712_-886148047","id":"20170302-205317_1581000322","dateCreated":"2018-03-01T15:34:39-0600","dateStarted":"2018-05-31T11:13:27-0500","dateFinished":"2018-05-31T11:13:29-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12990"},{"title":"LDA Topic Modeling  ","text":"// Set LDA parameters\nimport org.apache.spark.sql.types._\n\n//z.angularBind(\"num_topics\", \"10\") \n//z.angularBind(\"term_per_topic\", \"10\") \nval numTopics = z.angular(\"num_topics\").toString.toDouble.toInt\nval tpt =  z.angular(\"term_per_topic\").toString.toDouble.toInt\n\n//val numTopics = 10\nval lda = new LDA().setK(numTopics).setMaxIterations(100)\nval ldaModel = lda.run(documents)\nval topicIndices = ldaModel.describeTopics(maxTermsPerTopic = tpt)\nval topics = topicIndices.zipWithIndex.flatMap { case((terms, termWeights), id) => \n    terms.zip(termWeights).map { case (term, weight) =>\n       Row(id, vocabArray(term.toInt), weight )\n    }\n}\nval schema = StructType(\n  StructField(\"topic\", IntegerType, false) ::\n  StructField(\"word\", StringType, false) ::\n  StructField(\"Weight\", DoubleType,false) :: Nil)\n  \nval df =sqlContext.createDataFrame(sc.parallelize(topics), schema)\n\ndf.registerTempTable(\"result\");\nz.show(df)","user":"rhuang","dateUpdated":"2018-05-31T11:13:35-0500","config":{"editorSetting":{"language":"scala"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{"0":{"graph":{"mode":"table","height":808,"optionOpen":false,"setting":{"multiBarChart":{}},"commonSetting":{},"keys":[{"name":"topic","index":0,"aggr":"sum"}],"groups":[{"name":"word","index":1,"aggr":"sum"}],"values":[{"name":"Weight","index":2,"aggr":"sum"}]},"helium":{}},"1":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{}},"commonSetting":{},"keys":[{"name":"topic","index":0,"aggr":"sum"}],"groups":[{"name":"word","index":1,"aggr":"sum"}],"values":[{"name":"Weight","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.sql.types._\n\nnumTopics: Int = 5\n\ntpt: Int = 5\n\nlda: org.apache.spark.mllib.clustering.LDA = org.apache.spark.mllib.clustering.LDA@142d72be\n\nldaModel: org.apache.spark.mllib.clustering.LDAModel = org.apache.spark.mllib.clustering.DistributedLDAModel@4044ca20\n\ntopicIndices: Array[(Array[Int], Array[Double])] = Array((Array(15, 20, 23, 25, 29),Array(0.020610903014099822, 0.018684874603095546, 0.017761473492554344, 0.016689553326950914, 0.014261542561280108)), (Array(8, 11, 39, 45, 79),Array(0.024804828745449342, 0.02109705651065557, 0.012687268327622755, 0.010915564190416243, 0.008071528005914185)), (Array(12, 4, 5, 43, 13),Array(0.021349603241499322, 0.0198927285301137, 0.019499215789934663, 0.011702629169564013, 0.010058595245388416)), (Array(0, 1, 7, 16, 18),Array(0.049758668835708224, 0.04227452694603643, 0.02561191680683276, 0.019948676642925264, 0.018947600107951507)), (Array(2, 3, 6, 9, 17),Array(0.035581519798933224, 0.03555234346990279, 0.02699873117925201, 0.02206202970738919, 0.019261171139478197)))\ntopics: Array[org.apache.spark.sql.Row] = Array([0,sweden,0.020610903014099822], [0,cost,0.018684874603095546], [0,incident,0.017761473492554344], [0,family,0.016689553326950914], [0,security,0.014261542561280108], [1,white,0.024804828745449342], [1,house,0.02109705651065557], [1,golf,0.012687268327622755], [1,days,0.010915564190416243], [1,secret,0.008071528005914185], [2,mccain,0.021349603241499322], [2,media,0.0198927285301137], [2,press,0.019499215789934663], [2,john,0.011702629169564013], [2,america,0.010058595245388416], [3,rally,0.049758668835708224], [3,just,0.04227452694603643], [3,like,0.02561191680683276], [3,calls,0.019948676642925264], [3,florida,0.018947600107951507], [4,people,0.035581519798933224], [4,news,0.03555234346990279], [4,fake,0.02699873117925201], [4,want,0.022...\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(topic,IntegerType,false), StructField(word,StringType,false), StructField(Weight,DoubleType,false))\n\ndf: org.apache.spark.sql.DataFrame = [topic: int, word: string ... 1 more field]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"},{"type":"TABLE","data":"topic\tword\tWeight\n0\tsweden\t0.020610903014099822\n0\tcost\t0.018684874603095546\n0\tincident\t0.017761473492554344\n0\tfamily\t0.016689553326950914\n0\tsecurity\t0.014261542561280108\n1\twhite\t0.024804828745449342\n1\thouse\t0.02109705651065557\n1\tgolf\t0.012687268327622755\n1\tdays\t0.010915564190416243\n1\tsecret\t0.008071528005914185\n2\tmccain\t0.021349603241499322\n2\tmedia\t0.0198927285301137\n2\tpress\t0.019499215789934663\n2\tjohn\t0.011702629169564013\n2\tamerica\t0.010058595245388416\n3\trally\t0.049758668835708224\n3\tjust\t0.04227452694603643\n3\tlike\t0.02561191680683276\n3\tcalls\t0.019948676642925264\n3\tflorida\t0.018947600107951507\n4\tpeople\t0.035581519798933224\n4\tnews\t0.03555234346990279\n4\tfake\t0.02699873117925201\n4\twant\t0.02206202970738919\n4\tknow\t0.019261171139478197\n"}]},"apps":[],"jobName":"paragraph_1519940079712_-886148047","id":"20170302-205337_768402024","dateCreated":"2018-03-01T15:34:39-0600","dateStarted":"2018-05-31T11:13:35-0500","dateFinished":"2018-05-31T11:14:04-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:12991"},{"text":"%sh\nyarn node -list","user":"rhuang","dateUpdated":"2018-05-30T11:26:06-0500","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"18/05/30 11:26:08 INFO client.RMProxy: Connecting to ResourceManager at c252-101.wrangler.tacc.utexas.edu/129.114.58.144:8032\nTotal Nodes:2\n         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\nc252-103.wrangler.tacc.utexas.edu:40696\t        RUNNING\tc252-103.wrangler.tacc.utexas.edu:8042\t                           7\nc252-102.wrangler.tacc.utexas.edu:45710\t        RUNNING\tc252-102.wrangler.tacc.utexas.edu:8042\t                           6\n"}]},"apps":[],"jobName":"paragraph_1519940079723_-888841289","id":"20170309-114252_1556833967","dateCreated":"2018-03-01T15:34:39-0600","dateStarted":"2018-05-30T11:26:06-0500","dateFinished":"2018-05-30T11:26:09-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:13000"},{"text":"%sh\n","user":"rhuang","dateUpdated":"2018-05-31T11:35:32-0500","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1527784532876_-1499320137","id":"20180531-113532_2036719828","dateCreated":"2018-05-31T11:35:32-0500","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:13003"}],"name":"TwitterAnalysis","id":"2DA7SYAT4","angularObjects":{"2CSMDAYCD:shared_process":[],"2CSY3GZ6B:shared_process":[],"2CUQTWTYJ:shared_process":[],"2CT392RHH:shared_process":[],"2CUMBDKUU:shared_process":[],"2CSEGF9A4:shared_process":[],"2CSH7CR1V:shared_process":[],"2CTUNM1AY:shared_process":[],"2CSPA77M7:shared_process":[],"2CSRRGQ8B:shared_process":[],"2CTD4WC86:shared_process":[],"2CTX9VJA5:shared_process":[],"2CUT4WTTH:shared_process":[],"2CVBCRTNN:shared_process":[],"2CU5CWJZT:shared_process":[{"name":"stop_words","object":3,"noteId":"2DA7SYAT4","paragraphId":"20170302-205317_1581000322"},{"name":"term_per_topic","object":5,"noteId":"2DA7SYAT4","paragraphId":"20170302-205337_768402024"},{"name":"num_topics","object":5,"noteId":"2DA7SYAT4","paragraphId":"20170302-205337_768402024"},{"name":"input_f","object":{"inputfile":"file:/data/00791/xwj/tweets/tweets/parquet/2017/part-00096-3b88e4ea-3cb6-4efd-9e2e-64914ae08a19.snappy.parquet","from":"2016-01-19 15:00:00","end":"2018-01-19 20:00:00","stopwords_path":"file:/data/00791/xwj/tweets/tweets/stopwords.txt"},"noteId":"2DA7SYAT4","paragraphId":"20170302-163418_715093436"},{"name":"term_per_topic","object":"3","noteId":"2DA7SYAT4"},{"name":"input_f","object":"","noteId":"2DA7SYAT4"},{"name":"stop_words","object":"2","noteId":"2DA7SYAT4"},{"name":"num_topics","object":"3","noteId":"2DA7SYAT4"},{"name":"sentiment_output_path","object":"/home/03076/rhuang/senti","noteId":"2DA7SYAT4"}],"2CU69GJXR:shared_process":[],"2CSE7AUM4:shared_process":[],"2CVEQAPJH:shared_process":[],"2CVKEBQ8S:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}